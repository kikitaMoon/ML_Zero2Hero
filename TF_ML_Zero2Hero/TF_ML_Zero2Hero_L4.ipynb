{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e24ce26b-00a5-4115-99a7-a49e1656413b",
   "metadata": {},
   "source": [
    "# ML Zero to Hero - Lesson 4 - Build an image classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b0488-13c0-4f80-873c-c2c118d6ae1e",
   "metadata": {},
   "source": [
    "### Dataset Resources:\n",
    "- short link: http://bit.ly/tf-rps\n",
    "- complete link: https://laurencemoroney.com/datasets.html\n",
    "\n",
    "Images are seperated in different folders, so that no label is needed for this testing case.\n",
    "\n",
    "Datasets were dowloaded in `C:\\DeepLearning_Datasets\\RockPaperScissorsDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d12bbaa-e757-4374-8f72-c74f7f58205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae20fcc-dd00-4b81-8239-56bb629119e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2520 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = r'C:\\DeepLearning_Datasets\\RockPaperScissorsDataset\\rps'\n",
    "training_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size=(150,150),\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52acff7d-7db7-4520-8e9a-1abbe945e0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 372 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_DIR = r'C:\\DeepLearning_Datasets\\RockPaperScissorsDataset\\rps-test-set'\n",
    "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    target_size=(150,150),\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8752cb3c-c33b-4350-a5b2-0f9283ad25de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64,(3,3), activation='relu', input_shape=(150,150,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64,(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128,(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128,(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # Flatten the result to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),  #\n",
    "    \n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(3,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b59564c7-62d8-41b0-8b78-3cec5dfefb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 148, 148, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 3,473,475\n",
      "Trainable params: 3,473,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3ac7004-8461-4745-8687-da47ea91e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'rmsprop',\n",
    "              #metrics=['Accurary']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a5bc6e1-4df5-446f-8bf2-2c8e25e196c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "79/79 - 24s - loss: 0.6602 - val_loss: 0.6137\n",
      "Epoch 2/25\n",
      "79/79 - 23s - loss: 0.0593 - val_loss: 1.0133\n",
      "Epoch 3/25\n",
      "79/79 - 21s - loss: 0.0320 - val_loss: 1.1066\n",
      "Epoch 4/25\n",
      "79/79 - 23s - loss: 0.0254 - val_loss: 1.1388\n",
      "Epoch 5/25\n",
      "79/79 - 22s - loss: 0.0080 - val_loss: 1.8723\n",
      "Epoch 6/25\n",
      "79/79 - 23s - loss: 0.0104 - val_loss: 1.4119\n",
      "Epoch 7/25\n",
      "79/79 - 23s - loss: 0.0258 - val_loss: 1.3083\n",
      "Epoch 8/25\n",
      "79/79 - 21s - loss: 9.0188e-06 - val_loss: 1.6815\n",
      "Epoch 9/25\n",
      "79/79 - 23s - loss: 0.0053 - val_loss: 2.7387\n",
      "Epoch 10/25\n",
      "79/79 - 23s - loss: 7.4554e-05 - val_loss: 3.4745\n",
      "Epoch 11/25\n",
      "79/79 - 23s - loss: 0.0056 - val_loss: 2.9477\n",
      "Epoch 12/25\n",
      "79/79 - 23s - loss: 0.0370 - val_loss: 2.3845\n",
      "Epoch 13/25\n",
      "79/79 - 23s - loss: 0.0013 - val_loss: 1.9425\n",
      "Epoch 14/25\n",
      "79/79 - 23s - loss: 0.0271 - val_loss: 2.5030\n",
      "Epoch 15/25\n",
      "79/79 - 23s - loss: 5.2602e-05 - val_loss: 4.4975\n",
      "Epoch 16/25\n",
      "79/79 - 21s - loss: 0.0397 - val_loss: 2.3879\n",
      "Epoch 17/25\n",
      "79/79 - 22s - loss: 1.0832e-05 - val_loss: 2.8207\n",
      "Epoch 18/25\n",
      "79/79 - 23s - loss: 0.0078 - val_loss: 4.7105\n",
      "Epoch 19/25\n",
      "79/79 - 21s - loss: 7.9235e-08 - val_loss: 5.0390\n",
      "Epoch 20/25\n",
      "79/79 - 23s - loss: 0.0031 - val_loss: 6.5765\n",
      "Epoch 21/25\n",
      "79/79 - 22s - loss: 0.0324 - val_loss: 3.3108\n",
      "Epoch 22/25\n",
      "79/79 - 23s - loss: 7.9786e-07 - val_loss: 4.4534\n",
      "Epoch 23/25\n",
      "79/79 - 21s - loss: 6.5337e-07 - val_loss: 6.0156\n",
      "Epoch 24/25\n",
      "79/79 - 23s - loss: 0.0000e+00 - val_loss: 6.0158\n",
      "Epoch 25/25\n",
      "79/79 - 21s - loss: 0.0058 - val_loss: 3.6175\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(                           # fit_generator function is deprecated\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=validation_generator,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895687e-155c-446d-8303-248fc67f3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can not work yet,  as the input images coding are not prepared. \n",
    "\n",
    "validation_images = r'C:\\DeepLearning_Datasets\\RockPaperScissorsDataset\\rps-validation'\n",
    "testing_image = validation_images[0]\n",
    "testing_image_path = r'C:\\DeepLearning_Datasets\\Fashion_MNIST_sample\\image_0.png'\n",
    "classes = model.predict(images, batch_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
